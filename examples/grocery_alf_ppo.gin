# Training & playing the default goal task with Agent Learning Framework (Alf)
# python -m alf.bin.main --root_dir=~/tmp/gro_ppo --gin_file=grocery_alf_ppo.gin --alsologtostderr
# python -m alf.bin.main --root_dir=~/tmp/gro_ppo --gin_file=grocery_alf_ppo.gin --play --gin_param="on_policy_trainer.play.record_file='grocery.mp4'"

# Training with others task, 'kickball' for example:
# python -m alf.bin.main --root_dir=~/tmp/gro_ppo --gin_file=grocery_alf_ppo.gin --alsologtostderr --gin_param="GroceryGround.task_name='kickball'"

# If you are not recording video and observation does not contain image, you could add 'DISPLAY=null' to skip camera rendering, which will speedup the simulation a lot:
# DISPLAY=null python -m alf.bin.main ...

import alf.trainers.off_policy_trainer
import alf.algorithms.ppo_algorithm
import alf.algorithms.ppo_loss
import social_bot

# environment config
create_environment.env_name="SocialBot-GroceryGround-v0"
create_environment.num_parallel_environments=16
create_environment.env_load_fn=@suite_socialbot.load

# algorithm config

ActorCriticAlgorithm.loss_class=@PPOLoss
ActorCriticAlgorithm.gradient_clipping=0.5
ActorCriticAlgorithm.clip_by_global_norm=True

PPOLoss.entropy_regularization=5e-3
PPOLoss.gamma=0.99
PPOLoss.normalize_advantages=True
PPOLoss.td_lambda=0.95
PPOLoss.td_error_loss_fn=@element_wise_squared_loss

ActorDistributionNetwork.activation_fn=@tf.nn.tanh
ActorDistributionNetwork.continuous_projection_net=@NormalProjectionNetwork
NormalProjectionNetwork.init_means_output_factor=1e-10
NormalProjectionNetwork.std_bias_initializer_value=0.0
ValueNetwork.activation_fn=@tf.nn.tanh

create_ac_algorithm.actor_fc_layers=(128, 64)
create_ac_algorithm.value_fc_layers=(128, 64)
create_ac_algorithm.learning_rate=3e-4

# training config
TrainerConfig.trainer=@sync_off_policy_trainer
TrainerConfig.algorithm_ctor=@create_ppo_algorithm
TrainerConfig.mini_batch_length=1
TrainerConfig.unroll_length=1024
TrainerConfig.mini_batch_size=4096
TrainerConfig.num_iterations=1000000
TrainerConfig.summary_interval=1
TrainerConfig.num_updates_per_train_step=20
TrainerConfig.eval_interval=100
TrainerConfig.summarize_grads_and_vars=True
TrainerConfig.debug_summaries=True
TrainerConfig.checkpoint_interval=3

TFUniformReplayBuffer.max_length=2048
